// Copyright (c) 2025 Minio Inc. All rights reserved.
// Use of this source code is governed by a license that can be
// found in the LICENSE file.

#include "textflag.h"

TEXT ·updateAsm(SB), $0-40
	MOVD crc+0(FP), R0    // R0 = checksum
	MOVD p_base+8(FP), R1 // R1 = start pointer
	MOVD p_len+16(FP), R2 // R2 = length of buffer
	MVN  R0, R0
	ADD  $15, R1, R8
	AND  $-16, R8, R8
	SUB  R1, R8, R19
	SUB  R19, R2, R21
	CMP  $128, R21
	BLT  skip128

	ADD  R19, R1, R22
	AND  $-128, R21, R23
	LSR  $7, R21, R24
	MOVD R1, R20
	MOVD R19, R2
	CALL ·updateAsmSingle(SB)

	FLDPQ (R22), (F0, F1)
	FLDPQ 32(R22), (F2, F3)
	FLDPQ 64(R22), (F4, F5)
	FLDPQ 96(R22), (F6, F7)
	FMOVD R0, F8
	VMOVI $0, V9.B16
	VMOV  V9.D[0], V8.D[1]
	VEOR  V8.B16, V0.B16, V0.B16
	CMP   $1, R24
	BEQ   tail128
	ADD   R24<<7, R22, R8
	MOVD  $·const(SB), R11
	MOVD  112(R11), R10
	MOVD  120(R11), R11
	ADD   $128, R22, R9
	FMOVD R10, F8
	VDUP  R11, V9.D2

loop128:
	VPMULL  V0.D1, V8.D1, V10.Q1
	VPMULL2 V0.D2, V9.D2, V0.Q1
	FLDPQ   (R9), (F11, F12)
	VEOR3   V0.B16, V11.B16, V10.B16, V0.B16
	VPMULL  V1.D1, V8.D1, V10.Q1
	VPMULL2 V1.D2, V9.D2, V1.Q1
	VEOR3   V1.B16, V12.B16, V10.B16, V1.B16
	VPMULL  V2.D1, V8.D1, V10.Q1
	VPMULL2 V2.D2, V9.D2, V2.Q1
	FLDPQ   32(R9), (F11, F12)
	VEOR3   V2.B16, V11.B16, V10.B16, V2.B16
	VPMULL  V3.D1, V8.D1, V10.Q1
	VPMULL2 V3.D2, V9.D2, V3.Q1
	VEOR3   V3.B16, V12.B16, V10.B16, V3.B16
	VPMULL  V4.D1, V8.D1, V10.Q1
	VPMULL2 V4.D2, V9.D2, V4.Q1
	FLDPQ   64(R9), (F11, F12)
	VEOR3   V4.B16, V11.B16, V10.B16, V4.B16
	VPMULL  V5.D1, V8.D1, V10.Q1
	VPMULL2 V5.D2, V9.D2, V5.Q1
	VEOR3   V5.B16, V12.B16, V10.B16, V5.B16
	VPMULL  V6.D1, V8.D1, V10.Q1
	VPMULL2 V6.D2, V9.D2, V6.Q1
	FLDPQ   96(R9), (F11, F12)
	VEOR3   V6.B16, V11.B16, V10.B16, V6.B16
	VPMULL  V7.D1, V8.D1, V10.Q1
	VPMULL2 V7.D2, V9.D2, V7.Q1
	VEOR3   V7.B16, V12.B16, V10.B16, V7.B16
	ADD     $128, R9, R9
	CMP     R8, R9
	BNE     loop128

tail128:
	MOVD    $·const(SB), R9
	MOVD    (R9), R8
	FMOVD   R8, F11
	VPMULL  V0.D1, V11.D1, V11.Q1
	MOVD    8(R9), R8
	VDUP    R8, V12.D2
	VPMULL2 V0.D2, V12.D2, V0.Q1
	VEOR3   V0.B16, V7.B16, V11.B16, V7.B16
	MOVD    16(R9), R8
	FMOVD   R8, F11
	VPMULL  V1.D1, V11.D1, V11.Q1
	MOVD    24(R9), R8
	VDUP    R8, V12.D2
	VPMULL2 V1.D2, V12.D2, V1.Q1
	VEOR3   V1.B16, V11.B16, V7.B16, V1.B16
	MOVD    32(R9), R8
	FMOVD   R8, F11
	VPMULL  V2.D1, V11.D1, V11.Q1
	MOVD    40(R9), R8
	VDUP    R8, V12.D2
	VPMULL2 V2.D2, V12.D2, V2.Q1
	VEOR3   V2.B16, V11.B16, V1.B16, V2.B16
	MOVD    48(R9), R8
	FMOVD   R8, F11
	VPMULL  V3.D1, V11.D1, V11.Q1
	MOVD    56(R9), R8
	VDUP    R8, V12.D2
	VPMULL2 V3.D2, V12.D2, V3.Q1
	VEOR3   V3.B16, V11.B16, V2.B16, V3.B16
	MOVD    64(R9), R8
	FMOVD   R8, F11
	VPMULL  V4.D1, V11.D1, V11.Q1
	MOVD    72(R9), R8
	VDUP    R8, V12.D2
	VPMULL2 V4.D2, V12.D2, V4.Q1
	VEOR3   V4.B16, V11.B16, V3.B16, V4.B16
	MOVD    80(R9), R8
	FMOVD   R8, F11
	VPMULL  V5.D1, V11.D1, V11.Q1
	MOVD    88(R9), R8
	VDUP    R8, V12.D2
	VPMULL2 V5.D2, V12.D2, V5.Q1
	VEOR3   V5.B16, V11.B16, V4.B16, V5.B16
	MOVD    96(R9), R8
	FMOVD   R8, F11
	VPMULL  V6.D1, V11.D1, V11.Q1
	MOVD    104(R9), R8
	VDUP    R8, V12.D2
	VPMULL2 V6.D2, V12.D2, V6.Q1
	VEOR3   V6.B16, V11.B16, V5.B16, V6.B16
	FMOVD   R8, F5
	VPMULL  V6.D1, V5.D1, V5.Q1
	VDUP    V6.D[1], V6.D2
	VEOR    V5.B8, V6.B8, V6.B8
	MOVD    128(R9), R8
	FMOVD   R8, F4
	VPMULL  V4.D1, V6.D1, V6.Q1
	FMOVD   F6, R8
	MOVD    136(R9), R9
	FMOVD   R9, F4
	VPMULL  V4.D1, V6.D1, V6.Q1
	VEOR    V6.B16, V5.B16, V6.B16
	VMOV    V6.D[1], R9
	EOR     R8, R9, R0
	ADD     R23, R22, R1
	AND     $127, R21, R2

skip128:
	CALL ·updateAsmSingle(SB)
	MVN  R0, R0
	MOVD R0, checksum+32(FP)
	RET

TEXT ·updateAsmSingle(SB), $0-0
	AND  $15, R2, R8
	AND  $-16, R2, R9
	ADD  R9, R1, R9
	CMP  $16, R2
	BLO  skip16
	MOVD $·table0(SB), R10
	MOVD $·table1(SB), R11
	MOVD $·table2(SB), R12
	MOVD $·table3(SB), R13
	MOVD $·table4(SB), R14
	MOVD $·table5(SB), R15
	MOVD $·table6(SB), R16
	MOVD $·table7(SB), R17
	MOVD $·table8(SB), R2
	MOVD $·table9(SB), R3
	MOVD $·table10(SB), R4
	MOVD $·table11(SB), R5
	MOVD $·table12(SB), R6
	MOVD $·table13(SB), R7
	MOVD $·table14(SB), R19
	MOVD $·table15(SB), R20

loop16:
	MOVBU (R1), R21
	EORW  R0, R21, R21
	AND   $255, R21, R21
	MOVBU 15(R1), R22
	MOVD  (R10)(R22<<3), R22
	MOVBU 14(R1), R23
	MOVD  (R11)(R23<<3), R23
	EOR   R22, R23, R22
	MOVBU 13(R1), R23
	MOVD  (R12)(R23<<3), R23
	MOVBU 12(R1), R24
	MOVD  (R13)(R24<<3), R24
	EOR   R24, R23, R23
	EOR   R23, R22, R22
	MOVBU 11(R1), R23
	MOVD  (R14)(R23<<3), R23
	MOVBU 10(R1), R24
	MOVD  (R15)(R24<<3), R24
	EOR   R24, R23, R23
	MOVBU 9(R1), R24
	MOVD  (R16)(R24<<3), R24
	EOR   R24, R23, R23
	EOR   R23, R22, R22
	MOVBU 8(R1), R23
	MOVD  (R17)(R23<<3), R23
	MOVBU 7(R1), R24
	LSR   $56, R0, R25
	EORW  R25, R24, R24
	AND   $255, R24, R24
	MOVD  (R2)(R24<<3), R24
	EOR   R24, R23, R23
	EOR   R23, R22, R22
	MOVBU 6(R1), R23
	LSR   $48, R0, R24
	EORW  R24, R23, R23
	AND   $255, R23, R23
	MOVD  (R3)(R23<<3), R23
	MOVBU 5(R1), R24
	LSR   $40, R0, R25
	EORW  R25, R24, R24
	AND   $255, R24, R24
	MOVD  (R4)(R24<<3), R24
	EOR   R24, R23, R23
	MOVBU 4(R1), R24
	LSR   $32, R0, R25
	EORW  R25, R24, R24
	AND   $255, R24, R24
	MOVD  (R5)(R24<<3), R24
	EOR   R24, R23, R23
	EOR   R23, R22, R22
	MOVBU 3(R1), R23
	EORW  R0>>24, R23, R23
	AND   $255, R23, R23
	MOVD  (R6)(R23<<3), R23
	MOVBU 2(R1), R24
	EORW  R0>>16, R24, R24
	AND   $255, R24, R24
	MOVD  (R7)(R24<<3), R24
	EOR   R24, R23, R23
	MOVBU 1(R1), R24
	EORW  R0>>8, R24, R0
	AND   $255, R0, R0
	MOVD  (R19)(R0<<3), R0
	EOR   R0, R23, R0
	EOR   R0, R22, R0
	MOVD  (R20)(R21<<3), R21
	EOR   R21, R0, R0
	ADD   $16, R1, R1
	CMP   R9, R1
	BNE   loop16

skip16:
	CBZ  R8, done
	MOVD $·table0(SB), R10

loop1:
	MOVBU.P 1(R9), R11
	EORW    R0, R11, R11
	AND     $255, R11, R11
	MOVD    (R10)(R11<<3), R11
	EOR     R0>>8, R11, R0
	SUBS    $1, R8, R8
	BNE     loop1

done:
	RET

DATA ·const+0x000(SB)/8, $0xd083dd594d96319d // K_959
DATA ·const+0x008(SB)/8, $0x946588403d4adcbc // K_895
DATA ·const+0x010(SB)/8, $0x3c255f5ebc414423 // K_831
DATA ·const+0x018(SB)/8, $0x34f5a24e22d66e90 // K_767
DATA ·const+0x020(SB)/8, $0x7b0ab10dd0f809fe // K_703
DATA ·const+0x028(SB)/8, $0x03363823e6e791e5 // K_639
DATA ·const+0x030(SB)/8, $0x0c32cdb31e18a84a // K_575
DATA ·const+0x038(SB)/8, $0x62242240ace5045a // K_511
DATA ·const+0x040(SB)/8, $0xbdd7ac0ee1a4a0f0 // K_447
DATA ·const+0x048(SB)/8, $0xa3ffdc1fe8e82a8b // K_383
DATA ·const+0x050(SB)/8, $0xb0bc2e589204f500 // K_319
DATA ·const+0x058(SB)/8, $0xe1e0bb9d45d7a44c // K_255
DATA ·const+0x060(SB)/8, $0xeadc41fd2ba3d420 // K_191
DATA ·const+0x068(SB)/8, $0x21e9761e252621ac // K_127
DATA ·const+0x070(SB)/8, $0xa1ca681e733f9c40 // K_1087
DATA ·const+0x078(SB)/8, $0x5f852fb61e8d92dc // K_1023
DATA ·const+0x080(SB)/8, $0x27ecfa329aef9f77 // MU
DATA ·const+0x088(SB)/8, $0x34d926535897936b // POLY
GLOBL ·const(SB), (NOPTR+RODATA), $144
